
trainer:
  checkpoint_callback: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 1
        monitor: val_acc
        mode: max
        filename: 'DenseNet121-{epoch}-{step}-{val_acc:.4f}'
  gpus: 0, 1, 2, 3
  accelerator: ddp
  check_val_every_n_epoch: 1
  max_epochs: 200

model:
  model_name: 'densenet121'
  # training
  learning_rate: 0.1
  momentum: 0.9
  weight_decay: 5e-4
  milestones:
    - 100
    - 150
    - 200
data:
  data_adr: '../data'
  train_batch_size: 64
  eval_batch_size: 64
  num_workers: 4
